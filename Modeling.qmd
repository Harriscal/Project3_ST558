---
title: "Predictive Modeling of Diabetes Status Using Health Indicators"
format: html
editor: visual
editor_options: 
  chunk_output_type: console
---

## Introduction

In this section, we develop classification models to predict whether an individual has diabetes (`Diabetes_binary`) using health-related predictors from the 2015 BRFSS dataset.

Based on insights from the exploratory analysis, we focus on predictors such as `BMI`(Body Mass Index), `PhysActivty` (Physical Activity), and `HighBP` (High Blood Pressure). We will evaluate three types of models: logistic regression, classification trees, and random forests.

All models are fit using the `caret` package in R. Model performance is assessed using *log loss* as the primary evaluation metric, with *5-fold cross-validation* applied to select the best configuration within each model type. Final comparisons will be made on a held-out test set.

## Data Splitting

```{r Data Splitting}
#| message: false
#| warning: false
#load require libraries 
library(readr)
library(rsample)
library(caret)
library(dplyr)
library(yardstick)
library(rpart)
library(rpart.plot)
library(randomForest)
library(ranger)
library(ModelMetrics)


#read in data again (same as EDA.qmd)
diabetes <- read_csv("data/diabetes_binary_health_indicators_BRFSS2015.csv") |>
  as_tibble() |>
  mutate(
    Diabetes_binary = factor(Diabetes_binary, labels = c("No", "Yes")),
    HighBP = factor(HighBP, labels = c("No", "Yes")),
    PhysActivity = factor(PhysActivity, labels = c("No", "Yes"))
  )

#set seed for reproducibility
set.seed(123)

#split data: 70% training, 30% testing
split <- initial_split(diabetes, prop = 0.7, strata = Diabetes_binary)
train_data <- training(split)
test_data <- testing(split)
```

## Logistic Regression Models

Logistic regression is a classification method used when the response variable is binary. Instead of modeling the response directly, logistic regression models the *log-odds* (also called the logit) of the outcome as a linear combination of the predictors:

$$\log\left(\frac{P(Y = 1)}{1 - P(Y = 1)}\right) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p$$

This ensures that the predicted probabilities always lie between 0 and 1, which is a key property when modeling binary outcomes. In this analysis, the response variable is `Diabetes_binary`, which indicates whether or not an individual has diabetes (Yes or No), making logistic regression a natural and interpretable choice.

We use the `caret` package to fit three candidate logistic regression models using different sets of predictors identified during exploratory analysis. Each model is evaluated using *5-fold cross-validation* with *log loss* as the performance metric. Log loss penalizes incorrect predictions more severely when the model is overconfident, making it especially appropriate when evaluating predicted probabilities rather than simple classifications.

The three logistic regression models we fit are:

-   *Model 1:* BMI only\
-   *Model 2:* BMI and High Blood Pressure\
-   *Model 3:* BMI, High Blood Pressure, and Physical Activity

All models are fit using `caret::train()` with `method = "glm"` (for standard logistic regression) and `family = binomial`. The model with the *lowest average log loss* across the cross-validation folds is selected as the best model to carry forward.

```{r LRM Step 1: Setup for caret}
#define trainControl object for 5-fold CV + log loss
ctrl <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = mnLogLoss
)
```

```{r LRM Step 2: Define 3 logistic regression models}
#model 1: BMI only
log_model1 <- train(
  Diabetes_binary ~ BMI,
  data = train_data,
  method = "glm",
  family = "binomial",
  trControl = ctrl,
  metric = "logLoss"
)

#model 2: BMI + HighBP
log_model2 <- train(
  Diabetes_binary ~ BMI + HighBP,
  data = train_data,
  method = "glm",
  family = "binomial",
  trControl = ctrl,
  metric = "logLoss"
)

#model 3: BMI + HighBP + PhysActivity
log_model3 <- train(
  Diabetes_binary ~ BMI + HighBP + PhysActivity,
  data = train_data,
  method = "glm",
  family = "binomial",
  trControl = ctrl,
  metric = "logLoss"
)
```

```{r LRM Step 3: Compare Models (Log Loss)}
#display results in a data frame
model_results <- data.frame(
  Model = c("Model 1: BMI",
            "Model 2: BMI + HighBP",
            "Model 3: BMI + HighBP + PhysActivity"),
  LogLoss = c(
    min(log_model1$results$logLoss),
    min(log_model2$results$logLoss),
    min(log_model3$results$logLoss)
  )
)

model_results
```

The table above summarizes the log loss values for each of the three logistic regression models fit using 5-fold cross-validation.

-   *Model 1*, which includes only `BMI` as a predictor, has the highest log loss (0.384), indicating weaker performance.
-   *Model 2*, which adds `HighBP` to the model, improves prediction substantially, reducing the log loss to 0.357.
-   *Model 3*, which includes `BMI`, `HighBP`, and `PhysActivity`, achieves the *lowest log loss* (0.354), suggesting it has the best predictive accuracy of the three.

Thus, *Model 3* is selected as the best logistic regression model and will be retained for comparison against other model types (e.g., trees and random forests).

## Classification Tree Models

Classification trees are non-linear models that predict a categorical outcome by recursively splitting the predictor space into distinct and homogeneous regions. At each step, the algorithm chooses a predictor and a split point that maximizes class separation (typically using measures like Gini impurity or entropy). The result is a flowchart-like structure where observations are classified based on a sequence of decision rules.

Trees are appealing because they are easy to interpret and can naturally handle interactions between variables and non-linear relationships. They also do not require assumptions about the distribution of predictors. However, they can be prone to *overfitting*, which is why pruning the tree (via a complexity parameter, `cp`) is essential.

In this analysis, we fit a *classification tree* using the `rpart` method in the `caret` package. We tune the *complexity parameter (cp)* using 5-fold cross-validation and select the model that minimizes *log loss*. The predictors used in the tree are the same as in the best logistic regression model: BMI, High Blood Pressure, and Physical Activity.

```{r CTM}
#set up 5-fold CV with log loss
ctrl_tree <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = mnLogLoss
)

#grid of complexity parameter (cp) values to test
grid_tree <- expand.grid(cp = seq(0.001, 0.05, by = 0.005))

#fit classification tree model
set.seed(101)
tree_model <- train(
  Diabetes_binary ~ BMI + HighBP + PhysActivity,
  data = train_data,
  method = "rpart",
  trControl = ctrl_tree,
  metric = "logLoss",
  tuneGrid = grid_tree
)

# View results
tree_model$results
```

```{r CTM: Plot Best Classification Tree}
#extract the final tree model from caret object
final_tree <- tree_model$finalModel

#plot the tree
rpart.plot(final_tree,
           type = 2,         #type 2 = label all nodes
           extra = 104,      #shows fitted class + probability + % of obs
           under = TRUE,     #labels under the box
           faclen = 0,       #use full names for factor levels
           main = "Final Classification Tree for Predicting Diabetes")
```

The final classification tree plot above reveals that the model did not split the data at all. where it predicts every observation as "No" for diabetes. This outcome is likely due to the strong class imbalance in the training data, where about 86% of observations are non-diabetic.

The tuning grid of complexity parameters (cp values) did not affect the model performance, with all configurations resulting in the same log loss of approximately 0.4038. Since no split improved the log loss metric meaningfully, the algorithm defaulted to a root-only tree.

While classification trees are known for being interpretable and modeling interactions well, they can struggle with imbalanced data or when the splits do not provide sufficient gain under the loss function. This outcome suggests that classification trees may not be the best standalone model for this dataset, though they can still be useful within ensemble methods like random forests.

The final classification tree was fit using the formula:

$$Diabetes\_binary ∼ BMI + HighBP + PhysActivity $$
This model included all three predictors identified during the earlier modeling process. However, cross-validation using log-loss as the performance metric revealed that none of the potential splits offered a meaningful improvement. As a result, the optimal tree did not split at all, leading to a very simple stump model that predicts the majority class ("No") for all cases. This outcome is reflected in the tree plot and highlights one limitation of basic classification trees: they are prone to underfitting when signal is weak or thresholds are strict.

## Random Forest

Random Forest is an ensemble learning method that builds on the foundation of classification trees. Rather than fitting a single decision tree, a random forest grows many decision trees using random subsets of the data and predictors, and then averages their predictions (for regression) or takes a majority vote (for classification).

This approach helps address some of the major weaknesses of a single tree:

-   Trees are high variance models; a small change in the data can lead to a completely different tree.

-   Trees can easily overfit, especially if grown deep.

-   Random Forests mitigate this by averaging results across multiple trees, improving predictive accuracy and stability.

We also introduce randomness at each tree split (by selecting a random subset of predictors) which helps decorrelate the trees. This improves ensemble performance and is especially useful when predictors are correlated.

Given that our classification tree failed to split when using log-loss, a Random Forest may do better by combining many weak learners. We'll continue using log-loss and 5-fold cross-validation with the `caret` package, and will tune the number of variables (`mtry`) tried at each split.

```{r Random Forest}
#define the tuning grid for the ranger random forest model
rf_grid <- expand.grid(
  mtry = c(1, 2, 3),         #number of predictors considered at each split
  splitrule = "gini",        #use Gini impurity for classification (default)
  min.node.size = 10         #minimum number of observations in a terminal node (larger = faster)
)

#set seed for reproducibility
set.seed(101)

#train the random forest model using caret and ranger
rf_model <- train(
  Diabetes_binary ~ BMI + HighBP + PhysActivity,  #model formula
  data = train_data,                              #training data only
  method = "ranger",                              #use ranger implementation
  tuneGrid = rf_grid,                             #custom grid of tuning parameters
  metric = "logLoss",                             #evaluation metric for model performance
  maximize = FALSE,                               #minimize log-loss
  trControl = trainControl(
    method = "cv",                                #use cross-validation
    number = 5,                                   #5-fold CV
    classProbs = TRUE,                            #enable class probabilities for logLoss
    summaryFunction = mnLogLoss                   #specify custom summary function
  ),
  num.trees = 100                                 #reduce number of trees for faster training
)

#view cross-validation results for each combination of tuning parameters
rf_model$results
```

From above, the `ranger` engine was used from the `caret` package and trained models using 5-fold cross-validation. The log-loss metric was used to evaluate model performance. We set up a tuning grid for the number of predictors sampled at each split (`mtry`), using values from 1 to 3, since the model includes 3 total predictors.

Although `mtry = 2` had the lowest log-loss, its standard deviation was slightly higher than the others. Still, all models performed similarly, and `mtry = 2` was selected for the final model due to its superior average performance.

The final random forest model was also built using:

$$Diabetes\_binary ∼ BMI + HighBP + PhysActivity $$

The model was trained using the ranger method with 5-fold cross-validation and log-loss as the performance metric. To improve speed during rendering, we reduced the number of trees to 100 and limited the tuning grid to a few key values for mtry. Despite its complexity and ensemble structure, the model's performance on the test set was not superior to the simpler models, which is not uncommon in low-signal or imbalanced classification tasks.

## Final Model Selection

Now that we’ve trained and tuned three different model types; logistic regression, classification tree, and random forest. Using cross-validation on the training set, we shift our focus to evaluating their performance on the test set. The ultimate goal is to determine which model best generalizes to new, unseen data.

Each of the three models was trained using the same set of predictors: BMI, HighBP, and PhysActivity. This allowed for a fair comparison of the modeling approaches themselves while holding the input features constant.

To evaluate generalization performance, each model was assessed on the held-out test set using log-loss, which penalizes incorrect and overconfident predictions more heavily than accuracy. The results are summarized below:


```{r Final Model Selection: Compare Models}
#make predictions and evaluate on test set

#logistic Regression (best model was: BMI + HighBP + PhysActivity)
log_pred <- predict(log_model3, newdata = test_data, type = "prob")[, "Yes"]
log_loss_log <- logLoss(actual = test_data$Diabetes_binary, predicted = log_pred)

#classification Tree
tree_pred <- predict(tree_model, newdata = test_data, type = "prob")[, "Yes"]
log_loss_tree <- logLoss(actual = test_data$Diabetes_binary, predicted = tree_pred)

#random Forest
rf_pred <- predict(rf_model, newdata = test_data, type = "prob")[, "Yes"]
log_loss_rf <- logLoss(actual = test_data$Diabetes_binary, predicted = rf_pred)

#combine and display results
results <- tibble(
  Model = c("Logistic Regression", "Classification Tree", "Random Forest"),
  LogLoss = c(log_loss_log, log_loss_tree, log_loss_rf)
)

results
```

Despite being the simplest in structure, the Classification Tree achieved the lowest log-loss on the test set. While the tree did not perform splits during training due to lack of signal under cross-validation, it surprisingly generalized better than both logistic regression and random forest in this case.

This outcome highlights an important modeling lesson: more complex models do not always lead to better predictions, especially when the predictive signal is weak or the dataset is imbalanced. The classification tree model likely benefitted from conservative predictions that aligned well with the distribution of the test set.

Therefore, we select the Classification Tree as the final model for predicting diabetes status in this dataset.















